# data_preparation.py
import os
import shutil
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import chromadb # Import chromadb for direct client interaction if needed, though LangChain wrappers handle most.

def generate_rag_data(file_path="knowledge_base.txt"):
    """
    ç”Ÿæˆç”¨äºRAGçš„æ›´ä¸“ä¸šã€æ›´å…·åŒºåˆ†åº¦çš„ç¤ºä¾‹çŸ¥è¯†åº“æ•°æ®ã€‚
    èšç„¦äºâ€œæœªæ¥è®¡ç®—æ¶æ„â€é¢†åŸŸã€‚
    """
    data = r"""
    ## æœªæ¥è®¡ç®—æ¶æ„çš„æ¼”è¿›

    **1. é‡å­è®¡ç®— (Quantum Computing):**
    é‡å­è®¡ç®—åˆ©ç”¨é‡å­åŠ›å­¦ç°è±¡ï¼ˆå¦‚å åŠ å’Œçº ç¼ ï¼‰æ¥æ‰§è¡Œè®¡ç®—ã€‚ä¸ä¼ ç»Ÿä½ï¼ˆ0æˆ–1ï¼‰ä¸åŒï¼Œé‡å­ä½ï¼ˆqubitsï¼‰å¯ä»¥åŒæ—¶è¡¨ç¤º0å’Œ1ã€‚ä¸»è¦çš„é‡å­è®¡ç®—æ¨¡å‹åŒ…æ‹¬é—¨æ¨¡å‹ï¼ˆGate-based Quantum Computingï¼‰ã€é€€ç«æ¨¡å‹ï¼ˆQuantum Annealingï¼‰å’Œæ‹“æ‰‘é‡å­è®¡ç®—ï¼ˆTopological Quantum Computingï¼‰ã€‚IBMçš„Qiskitå’ŒGoogleçš„Cirqæ˜¯æµè¡Œçš„é‡å­ç¼–ç¨‹æ¡†æ¶ã€‚é‡å­éœ¸æƒï¼ˆQuantum Supremacyï¼‰æŒ‡çš„æ˜¯é‡å­è®¡ç®—æœºåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¶…è¶Šæœ€å¼ºå¤§çš„ç»å…¸è®¡ç®—æœºçš„èƒ½åŠ›ã€‚

    **2. ç¥ç»å½¢æ€è®¡ç®— (Neuromorphic Computing):**
    ç¥ç»å½¢æ€è®¡ç®—æ—¨åœ¨æ¨¡ä»¿äººè„‘çš„ç»“æ„å’ŒåŠŸèƒ½ï¼Œä»¥å®ç°æ›´é«˜çš„èƒ½æ•ˆå’Œå¹¶è¡Œå¤„ç†èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒç»„ä»¶æ˜¯â€œç¥ç»å…ƒâ€å’Œâ€œçªè§¦â€ï¼Œå®ƒä»¬å¯ä»¥ç›´æ¥åœ¨ç¡¬ä»¶å±‚é¢æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç½‘ç»œçš„è¡Œä¸ºã€‚IBMçš„TrueNorthèŠ¯ç‰‡æ˜¯ç¥ç»å½¢æ€è®¡ç®—çš„ä¸€ä¸ªæ—©æœŸä»£è¡¨ï¼Œè€Œè‹±ç‰¹å°”çš„Loihiç³»åˆ—èŠ¯ç‰‡åˆ™è¿›ä¸€æ­¥æ¨åŠ¨äº†è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œä¸“æ³¨äºäº‹ä»¶é©±åŠ¨çš„ç¨€ç–å¤„ç†ã€‚è¿™ç§æ¶æ„ç‰¹åˆ«é€‚ç”¨äºäººå·¥æ™ºèƒ½å’Œè¾¹ç¼˜è®¡ç®—åœºæ™¯ã€‚

    **3. å…‰å­è®¡ç®— (Photonic Computing):**
    å…‰å­è®¡ç®—ä½¿ç”¨å…‰å­ï¼ˆè€Œéç”µå­ï¼‰æ¥ä¼ è¾“å’Œå¤„ç†ä¿¡æ¯ã€‚ç”±äºå…‰å­é€Ÿåº¦å¿«ã€èƒ½è€—ä½ä¸”ä¸å—ç”µç£å¹²æ‰°ï¼Œå…‰å­è®¡ç®—åœ¨æŸäº›ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚é«˜é€Ÿæ•°æ®ä¼ è¾“ã€æ¨¡æ‹Ÿè®¡ç®—å’Œçº¿æ€§ä»£æ•°è¿ç®—ï¼‰ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç¡…å…‰å­å­¦æŠ€æœ¯æ˜¯å®ç°é›†æˆå…‰å­è®¡ç®—çš„å…³é”®ï¼Œå®ƒå…è®¸åœ¨ç¡…åŸºèŠ¯ç‰‡ä¸Šé›†æˆå…‰å­¦å…ƒä»¶ã€‚

    **4. ç±»è„‘è®¡ç®— (Brain-inspired Computing):**
    ç±»è„‘è®¡ç®—æ˜¯ä¸€ä¸ªæ›´å¹¿æ³›çš„æ¦‚å¿µï¼Œå®ƒä¸ä»…é™äºç¡¬ä»¶å±‚é¢æ¨¡ä»¿å¤§è„‘ç»“æ„ï¼Œè¿˜åŒ…æ‹¬ç®—æ³•å’Œè½¯ä»¶å±‚é¢å€Ÿé‰´å¤§è„‘çš„å­¦ä¹ å’Œè®¤çŸ¥æœºåˆ¶ã€‚è¿™ä¸ç¥ç»å½¢æ€è®¡ç®—æœ‰æ‰€é‡å ï¼Œä½†æ›´å¼ºè°ƒç®—æ³•å±‚é¢çš„åˆ›æ–°ï¼Œå¦‚è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSpiking Neural Networks, SNNsï¼‰å’Œè®¤çŸ¥è®¡ç®—æ¨¡å‹ã€‚

    **5. è¾¹ç¼˜è®¡ç®—ä¸é›¾è®¡ç®— (Edge and Fog Computing):**
    éšç€ç‰©è”ç½‘(IoT)è®¾å¤‡çš„æ™®åŠï¼Œå°†æ•°æ®å¤„ç†èƒ½åŠ›æ¨å‘æ•°æ®æºé™„è¿‘çš„è¾¹ç¼˜è®¾å¤‡å˜å¾—è‡³å…³é‡è¦ã€‚è¾¹ç¼˜è®¡ç®—å‡å°‘äº†æ•°æ®ä¼ è¾“çš„å»¶è¿Ÿå’Œå¸¦å®½éœ€æ±‚ï¼Œæé«˜äº†å“åº”é€Ÿåº¦å’Œå®‰å…¨æ€§ã€‚é›¾è®¡ç®—æ˜¯è¾¹ç¼˜è®¡ç®—çš„ä¸€ç§å»¶ä¼¸ï¼Œå®ƒåœ¨äº‘ç«¯å’Œè¾¹ç¼˜è®¾å¤‡ä¹‹é—´å¼•å…¥äº†ä¸€ä¸ªä¸­é—´å±‚ï¼Œæä¾›æ›´å¹¿æ³›çš„è®¡ç®—ã€å­˜å‚¨å’Œç½‘ç»œæœåŠ¡ã€‚

    **6. å¯é€†è®¡ç®— (Reversible Computing):**
    å¯é€†è®¡ç®—æ˜¯ä¸€ç§è®¡ç®—èŒƒå¼ï¼Œå…¶è®¡ç®—è¿‡ç¨‹åœ¨é€»è¾‘ä¸Šæ˜¯å¯é€†çš„ï¼Œè¿™æ„å‘³ç€å®ƒåœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¸æŸå¤±ä»»ä½•ä¿¡æ¯ã€‚ç†è®ºä¸Šï¼Œå¯é€†è®¡ç®—å¯ä»¥åœ¨ä¸äº§ç”Ÿçƒ­é‡çš„æƒ…å†µä¸‹æ‰§è¡Œï¼Œè¿™å¯¹äºè¶…ä½åŠŸè€—è®¡ç®—å’Œé¿å…ç‰©ç†é™åˆ¶è‡³å…³é‡è¦ã€‚LandaueråŸç†æŒ‡å‡ºï¼Œæ¯æ¬¡æ“¦é™¤ä¸€ä½ä¿¡æ¯è‡³å°‘ä¼šè€—æ•£ $kT \ln 2$ çš„èƒ½é‡ï¼Œå¯é€†è®¡ç®—æ—¨åœ¨è§„é¿è¿™ä¸€é™åˆ¶ã€‚

    **7. é‡å­AI (Quantum AI):**
    é‡å­AIæ˜¯é‡å­è®¡ç®—ä¸äººå·¥æ™ºèƒ½çš„äº¤å‰é¢†åŸŸã€‚å®ƒåˆ©ç”¨é‡å­ç®—æ³•æ¥åŠ é€Ÿæœºå™¨å­¦ä¹ ä»»åŠ¡ï¼ˆå¦‚é‡å­æœºå™¨å­¦ä¹ ã€é‡å­ä¼˜åŒ–ï¼‰æˆ–æ¨¡æ‹Ÿå¤æ‚çš„ç¥ç»ç½‘ç»œã€‚å…¸å‹çš„åº”ç”¨åŒ…æ‹¬é‡å­æ”¯æŒå‘é‡æœº (QSVM) å’Œé‡å­ç¥ç»ç½‘ç»œ (QNN)ã€‚

    **é€šç”¨æ¦‚å¿µè¡¥å……:**
    * **LangChain**: ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ï¼Œå®ƒæä¾›äº†æ„å»ºå¤æ‚LLMåº”ç”¨æ‰€éœ€çš„ç»„ä»¶å’Œæ¥å£ã€‚
    * **Streamlit**: ç”¨äºå¿«é€Ÿæ„å»ºå’Œåˆ†äº«æ•°æ®åº”ç”¨çš„Pythonåº“ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿç”¨çº¯Pythonä»£ç åˆ›å»ºäº¤äº’å¼Webåº”ç”¨ã€‚
    * **ChromaDB**: ä¸€ä¸ªå¼€æºçš„åµŒå…¥å¼æ•°æ®åº“ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢åµŒå…¥å‘é‡ï¼Œå¸¸ä¸è¯­è¨€æ¨¡å‹ç»“åˆå®ç°é«˜æ•ˆè¯­ä¹‰æœç´¢å’ŒRAGåŠŸèƒ½ã€‚
    """
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(data)
    print(f"âœ… çŸ¥è¯†åº“æ•°æ®å·²ç”Ÿæˆå¹¶ä¿å­˜åˆ° {file_path}")

def _get_embedding_function():
    """Helper to load the embedding model."""
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-small-zh",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… HuggingFace Embedding æ¨¡å‹ (BAAI/bge-small-zh) åŠ è½½æˆåŠŸã€‚")
        return embeddings
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£… 'sentence-transformers' åº“ï¼Œå¹¶ä¸”ç½‘ç»œè¿æ¥æ­£å¸¸ä»¥ä¸‹è½½æ¨¡å‹ã€‚")
        raise

def load_and_vectorize_data(file_path="knowledge_base.txt", persist_directory="./chroma_db", force_rebuild=False):
    """
    ä»æ–‡æœ¬æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œåˆ†å‰²ï¼Œå¹¶åˆ›å»ºæˆ–åŠ è½½Chromaå‘é‡å­˜å‚¨ã€‚
    force_rebuild=True ä¼šå¼ºåˆ¶åˆ é™¤ç°æœ‰å‘é‡å­˜å‚¨å¹¶é‡æ–°åˆ›å»ºã€‚
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ '{file_path}' ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ generate_rag_dataã€‚")

    embeddings = _get_embedding_function()
    vectorstore = None
    
    # Decide if we should try to load or if we must rebuild
    # We rebuild if force_rebuild is True OR if the directory doesn't exist/is empty
    should_rebuild = force_rebuild or \
                     not os.path.exists(persist_directory) or \
                     len(os.listdir(persist_directory)) == 0

    if should_rebuild:
        # If rebuilding, first clean up any existing directory
        if os.path.exists(persist_directory):
            print(f"ğŸ”„ æ£€æµ‹åˆ°éœ€è¦é‡å»ºå‘é‡å­˜å‚¨ï¼Œæ­£åœ¨åˆ é™¤æ—§ç›®å½•: {persist_directory}")
            shutil.rmtree(persist_directory)
            print("âœ… æ—§ç›®å½•åˆ é™¤æˆåŠŸã€‚")

        print(f"ğŸ”„ æ­£åœ¨ä» {file_path} è¯»å–æ–‡æ¡£å¹¶åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨...")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        print(f"âœ… å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ã€‚")
        if documents:
            print(f"   ç¬¬ä¸€ä¸ªæ–‡æ¡£å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰:\n---START---\n{documents[0].page_content[:200]}...\n---END---")
        else:
            print("   âš ï¸ æœªåŠ è½½åˆ°ä»»ä½•æ–‡æ¡£å†…å®¹ï¼è¯·æ£€æŸ¥ 'knowledge_base.txt' æ–‡ä»¶ã€‚")
            raise ValueError("çŸ¥è¯†åº“æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•åŠ è½½ã€‚")

        text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = text_splitter.split_documents(documents)
        print(f"âœ… æ–‡æ¡£åˆ†å‰²åç”Ÿæˆäº† {len(docs)} ä¸ªæ–‡æœ¬å—ã€‚")
        if docs:
            print(f"   ç¬¬ä¸€ä¸ªæ–‡æœ¬å—å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰:\n---START---\n{docs[0].page_content[:200]}...\n---END---")
        else:
            print("   âŒ æœªç”Ÿæˆä»»ä½•æ–‡æœ¬å—ï¼è¯·æ£€æŸ¥æ–‡æœ¬åˆ†å‰²å™¨é…ç½®æˆ–æ–‡æ¡£å†…å®¹ã€‚")
            raise ValueError("æ–‡æœ¬åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆä»»ä½•æ–‡æœ¬å—ã€‚")

        print("ğŸ”„ æ­£åœ¨åˆ›å»º Chroma å‘é‡å­˜å‚¨å¹¶ç”ŸæˆåµŒå…¥å‘é‡...")
        # IMPORTANT: This is the ONLY place Chroma.from_documents is called.
        # It creates the DB and implicitly persists it to persist_directory.
        vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=embeddings,
            persist_directory=persist_directory,
            collection_metadata={"hnsw:space": "cosine"} # Match your insert_Vector.py
        )
        print(f"âœ… Chroma å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸã€‚")

        # Call persist() directly after from_documents.
        # While from_documents often implicitly persists, explicit call ensures consistency.
        vectorstore.persist()
        print(f"âœ… å‘é‡å­˜å‚¨å·²æŒä¹…åŒ–åˆ° {persist_directory}ã€‚æ€»è®¡ {vectorstore._collection.count()} ä¸ªæ¡ç›®ã€‚")
        
    else: # should_rebuild is False, so try to load existing
        print(f"âœ… æ£€æµ‹åˆ° {persist_directory} æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œå°è¯•ä»æŒä¹…åŒ–æ•°æ®åŠ è½½ã€‚")
        try:
            # IMPORTANT: This is the ONLY place Chroma is loaded (not created from documents).
            vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
            count = vectorstore._collection.count()
            if count > 0:
                print(f"âœ… å‘é‡å­˜å‚¨å·²ä»æŒä¹…åŒ–æ•°æ®åŠ è½½ã€‚æ€»è®¡ {count} ä¸ªæ¡ç›®ã€‚")
            else:
                # This scenario (directory exists but empty) implies corruption or incomplete write
                print(f"âš ï¸ {persist_directory} æ–‡ä»¶å¤¹å­˜åœ¨ä½†ä¸ºç©ºï¼Œæˆ–æ•°æ®ä¸å®Œæ•´ã€‚æ¨èé‡æ–°ç”Ÿæˆã€‚")
                # Fallback: force a rebuild if loaded DB is empty
                return load_and_vectorize_data(file_path, persist_directory, force_rebuild=True) 
        except Exception as e:
            print(f"âŒ ä»æŒä¹…åŒ–æ•°æ®åŠ è½½ ChromaDB å¤±è´¥: {e}ã€‚æ¨èé‡æ–°ç”Ÿæˆã€‚")
            # Fallback: force a rebuild if loading fails
            return load_and_vectorize_data(file_path, persist_directory, force_rebuild=True)

    return vectorstore

if __name__ == "__main__":
    print("--- æ­£åœ¨æ‰§è¡Œ data_preparation.py ---")
    generate_rag_data()
    try:
        # Running directly, you might want to force_rebuild=True for initial setup
        vectorstore_test = load_and_vectorize_data(force_rebuild=True) # Force rebuild for testing
        print("\n--- data_preparation.py æ‰§è¡Œå®Œæˆ ---")
    except Exception as e:
        print(f"\nâŒ data_preparation.py å‘é‡åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")